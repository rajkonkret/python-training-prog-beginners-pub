{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnxSYhfbLiMG"
      },
      "source": [
        "## Wprowadzenie do Pytorcha\n",
        "Zanim przejdziemy do sieci neuronowych przyjrzymy się bilżej bibliotece PyTorch. Jest to biblioteka open source tworzona i wspierana przez firmę Facebook. Umożliwia ona sprawne zbudowanie modeli i ich trenowanie dzięki m. in. wbudowanym mechanizmom automatycznego różniczkowania.\n",
        "\n",
        "Konkurencyjnym rozwiązaniem są biblioteki Tensorflow oraz Keras (ta druga jest wysokopoziomowym API przeznaczonym m. in. do budowania sieci neuronowych). Jest to rozwiązanie wspierane przez Google, którego pierwsza wersja została wydana w 2015 roku.\n",
        "\n",
        "Można zauważyć, że Keras jest częściej stosowany do celów produkcyjnych, a PyTorch do celów naukowych, lecz wiele zależy od osobistych preferencji.\n",
        "\n",
        "Jeżeli chcesz się dowiedzieć więcej o automatycznym różniczkowaniu, to zajrzyj tutaj (notebook w języku angielskim): https://drive.google.com/file/d/1nZFPdZIN6YtzSFqmnMiZmkqgPzjxifDY/view?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqn5UlojE2nT"
      },
      "source": [
        "## Metoda gradientowego spadku - przypomnienie\n",
        "\n",
        "![](https://miro.medium.com/max/700/1*yasmQ5kvlmbYMe8eDkyl6w.png)\n",
        "\n",
        "Źródło ilustracji: https://miro.medium.com/max/700/1*yasmQ5kvlmbYMe8eDkyl6w.png\n",
        "\n",
        "Niech: <br>\n",
        " $X^t \\in \\mathbb{R}^{n}$, <br>\n",
        " $ f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$ <br>\n",
        " Krok gradientowego spadku dla kroku **t** wygląda następująco:\n",
        "\n",
        "$$ X^{t + 1} = X^{t} - \\eta \\frac{\\partial f(X^t)}{\\partial X} = X^{t} - \\eta \\cdot grad (f(X^t)) $$\n",
        "\n",
        "Gdzie $\\eta$ jest parametrem zwanym learning rate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM2TeocuRXzx"
      },
      "source": [
        "## Podstawowe operacje na tensorach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ia1bpC5Pe8x",
        "outputId": "e378b1ba-206e-4e0e-bfc5-317131bf81d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.Tensor([1,2,3])\n",
        "print(3 * a)\n",
        "print(a ** a)\n",
        "print(a * a)\n",
        "print(a @ a)\n",
        "b = torch.Tensor([[1,2,3], [4,5,6], [10, 11, 12]])\n",
        "\n",
        "print(b[1:3,0:2])\n",
        "\n",
        "print(b + a)\n",
        "\n",
        "print(a[1].item())\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 6., 9.])\n",
            "tensor([ 1.,  4., 27.])\n",
            "tensor([1., 4., 9.])\n",
            "tensor(14.)\n",
            "tensor([[ 4.,  5.],\n",
            "        [10., 11.]])\n",
            "tensor([[ 2.,  4.,  6.],\n",
            "        [ 5.,  7.,  9.],\n",
            "        [11., 13., 15.]])\n",
            "2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryIXQ-ojRgYE"
      },
      "source": [
        "## Przykład - regresja liniowa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ad9jyWhPNk96",
        "outputId": "78740d9f-976d-4313-a2d2-0081f6475ae5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "def linear_model(x, W, b):\n",
        "    return x*W + b\n",
        "\n",
        "data, targets = torch.Tensor([[1],[2],[3]]), torch.Tensor([3,5,7])\n",
        "\n",
        "W = Variable(torch.randn(1), requires_grad=True)\n",
        "b = Variable(torch.randn(1), requires_grad=True)\n",
        "epochs = 10\n",
        "\n",
        "optimizer = optim.SGD([W, b], lr=0.000001)\n",
        "# optimizer = optim.Adam([W, b])\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for sample, target in zip(data, targets):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = linear_model(sample, W, b)\n",
        "        loss = (output - target) ** 2\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(epoch_loss)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "98.87383460998535\n",
            "98.86727905273438\n",
            "98.86072206497192\n",
            "98.85416650772095\n",
            "98.8476152420044\n",
            "98.84106016159058\n",
            "98.8345046043396\n",
            "98.82795333862305\n",
            "98.82139825820923\n",
            "98.81484794616699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao5OlkhURvnD"
      },
      "source": [
        "## Zadanie 1.\n",
        "\n",
        "Po uruchumieniu algorytmu funkcja kosztu dosyć wolno spada. Potrzeba wielu epok aby osiągnąć satysfakcjonujący wynik. Zaproponuj rozwiązanie, które przyspieszy uczenie się.\n",
        "\n",
        "Co się stanie po zakomentowaniu optimizer.zero_grad()? Dlaczego wykorzytujemy tę funkcję?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z_tLIEgOR3X"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n002tZEmQQ9K"
      },
      "source": [
        "## Perceptron - najprostsza sieć neuronowa\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/8/8c/Perceptron_moj.png)\n",
        "\n",
        "Najprostsza sieć neuronowa składa się pewnej ilości wag ($w_1,...,w_n$) i funkcji aktywacji.\n",
        "\n",
        "Wyobraźmy sobie następujący problem: mamy dane dotyczące pacjentów - wyniki badań, wiek, wagę itp. Chcemy stworzyć klasyfikator - funkcję, która na wejście przyjmie dane pancjentów, a na wyjście zwróci 2 możliwe wartości: 0 - pacjent chory, 1 - pacjent zdrowy.\n",
        "\n",
        "Nasze dane możemy oznaczyć jako $x_1,...,x_n$. Nasza sieć zadziałałaby następująco: Przemnażamy każdą z tych danych pacjenta przez odpowiadającą mu wagę ($w_i$) i sumujemy. Jeżeli suma jest powyżej pewnego progu zwracamy 1, a jeżeli poniżej to 0.\n",
        "\n",
        "W ogólności:\n",
        "$$f(x)=a\\left(\\sum_{i=1}^nw_ix_i+b\\right)$$\n",
        "\n",
        "Gdzie a jest tzw. funkcją aktywacji.\n",
        "\n",
        "Pojawia się jeszcze pytanie - jak dobrać wagi?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tzpCzKOZGy_"
      },
      "source": [
        "## Zadanie 2.\n",
        "Uzupełnij implementację jednowarstwowego perceptronu z funkcją aktywacji sigmoid.\n",
        "\n",
        "Funkcja sigmoidalna dana jest wzorem $f(x)=\\frac{1}{1+e^{-x}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-qpPzmXZFzg",
        "outputId": "23daba16-8b40-4cb3-88cc-46b589d5d8fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1+torch.exp(-x))\n",
        "\n",
        "data = torch.Tensor([[1,-2], [-1, 2], [1,1], [-4,-7]])\n",
        "values = torch.Tensor([1,0,0,1])\n",
        "weights = Variable(torch.randn(2), requires_grad=True)\n",
        "bias = Variable(torch.randn(1), requires_grad=True)\n",
        "\n",
        "def network_model(x, weights, bias):\n",
        "    return sigmoid(weights @ x + bias)\n",
        "\n",
        "epochs = 10\n",
        "optimizer = optim.Adam([weights, bias], lr=0.01)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for sample, target in zip(data, values):\n",
        "        optimizer.zero_grad()\n",
        "        output = network_model(sample, weights, bias)\n",
        "        loss = (output - target) ** 2\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(weights)\n",
        "    print(bias)\n",
        "    print(epoch_loss)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.4930,  0.0228], requires_grad=True)\n",
            "tensor([0.0034], requires_grad=True)\n",
            "1.4340365286534507\n",
            "tensor([-1.4670, -0.0081], requires_grad=True)\n",
            "tensor([0.0022], requires_grad=True)\n",
            "1.385756105407836\n",
            "tensor([-1.4416, -0.0383], requires_grad=True)\n",
            "tensor([-0.0001], requires_grad=True)\n",
            "1.3421916798642997\n",
            "tensor([-1.4164, -0.0684], requires_grad=True)\n",
            "tensor([-0.0028], requires_grad=True)\n",
            "1.2979824996218667\n",
            "tensor([-1.3910, -0.0987], requires_grad=True)\n",
            "tensor([-0.0056], requires_grad=True)\n",
            "1.2524443078768854\n",
            "tensor([-1.3653, -0.1291], requires_grad=True)\n",
            "tensor([-0.0084], requires_grad=True)\n",
            "1.205395786950021\n",
            "tensor([-1.3394, -0.1596], requires_grad=True)\n",
            "tensor([-0.0113], requires_grad=True)\n",
            "1.1568300060150705\n",
            "tensor([-1.3132, -0.1904], requires_grad=True)\n",
            "tensor([-0.0142], requires_grad=True)\n",
            "1.106846161400199\n",
            "tensor([-1.2867, -0.2214], requires_grad=True)\n",
            "tensor([-0.0170], requires_grad=True)\n",
            "1.0556263894029598\n",
            "tensor([-1.2600, -0.2524], requires_grad=True)\n",
            "tensor([-0.0198], requires_grad=True)\n",
            "1.003424607631814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv_8BX8XQW9v"
      },
      "source": [
        "## Twierdzenie o uniwersalnej aproksymacji\n",
        "\n",
        "Tw. (Cybenko, 1989). Niech F będzie ciągłą funkcją sigmoidalną. Skończone sumy postaci $$G(x)=\\sum_{j=1}^Nα_jF(w_jx+θ_j)$$ są gęste w przestrzeni funkcji ciągłych n zmiennych na kostce jednostkowej.\n",
        "\n",
        "Innymi słowy - mając sieć z jedną warstwą z sigmoidalną funkcją aktywacji możemy wraz dowolnie dużą ilością neuronów w tej warstwie przbliżać funkcję ciągłą z wartościami [0,1] z dowolnie dużą dokładnością.\n",
        "\n",
        "W bardziej ogólnej wersji to twierdzenie dotyczy funkcji aktywacji F które są ciągłe i ograniczone. W szczególności mając wielowarstwową sieć o funkcjach liniowych w ostatniej warstwie (ale ograniczone i ciągłe między warstwami) możemy przybliżać dowolną funkcję ciągłą.\n",
        "\n",
        "Jednym z nowszych odkryć (Patrick Kidger, Terry Lyons, 2020) jest twierdzenie, że to dotyczy ciągłych funkcji aktywacji nieafinicznych o ciągłej pochodnej w co najmniej jednym punkcie, przy czym istnieje punkt mający pochodną różną od 0.\n",
        "\n",
        "\n",
        "\n",
        "![](https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-hqc5_gd5Iz"
      },
      "source": [
        "## Funkcje aktywacji\n",
        "Niegdyś najczęściej używaną funkcją aktywacji był sigmoid. Obecnie najczęściej używaną funkcją aktywacji jest ReLu. Wyraża się wzorem:\n",
        "$$f(x)=\\max(0,x)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-Q4IS4ce7ha",
        "outputId": "97146fdc-4203-4f5a-c1b6-822e991f63f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc0 = nn.Linear(28*28, 920)\n",
        "        self.fc1 = nn.Linear(920, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    log_interval = 100\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=100)\n",
        "\n",
        "model = Network().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test(model, test_loader)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102285324.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 18974889.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26204598.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14688148.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.280176\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.236152\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.287113\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.286522\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.186919\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.234267\n",
            "\n",
            "Test set: Average loss: 0.2626, Accuracy: 9312/10000 (93%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.116797\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.285550\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.169965\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.160395\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.068230\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.118257\n",
            "\n",
            "Test set: Average loss: 0.2190, Accuracy: 9489/10000 (95%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.143920\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.361982\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.267700\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.269744\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.136613\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.143935\n",
            "\n",
            "Test set: Average loss: 0.2386, Accuracy: 9399/10000 (94%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeVhj3nZigV4"
      },
      "source": [
        "## Zadanie 3.\n",
        "Uruchom obliczenia na karcie graficznej. Czy widoczne jest przyspieszenie?\n",
        "\n",
        "Pamiętaj! Aby model działał na karcie graficznej musisz go tam umieścić za pomocą polecenia .cuda(). Musisz wywołać to na klasie modelu jak i na tenasorach danych!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-mEyxV2pn-e"
      },
      "source": [
        "## Zadanie 4.\n",
        "Stwórz w używanym przez siebie edytorze graficznym cyfrę podobną do zestawu mnist, a następnie dokonaj predykcji za pomocą swojej sieci. Czy predykcja jest poprawna?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhfgClVa1vry",
        "outputId": "b2c54625-15d3-4a8c-d944-04c5135a47d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "os.getcwd()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxba311O1x3g",
        "outputId": "c89a6b76-e0eb-48f3-b864-c7effbf244af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img = Image.open(\"cyfra.png\").convert('L')\n",
        "plt.imshow(np.reshape(img, (28,28)), cmap=plt.cm.gray)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd9ccbafb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMIUlEQVR4nO3dX4hcZx3G8eexKixV2MRiGmo0KrkRwSoheBGkIsqam7Q3xVwlRVy7WLAgaKgXSS9ki1qLV0m2NE0U3VSoYi4EjUGI3ki3pbZpa9vYpjRhm1hz0fQi1DY/L+bETtKdM5s558w52d/3A8vMnHdmzo+TPj1/3nnP64gQgJXvfW0XAGA8CDuQBGEHkiDsQBKEHUji/eNcmW0u/QMNiwgvtbzSnt32lO3nbZ+wvbPKdwFolkftZ7d9naQXJH1V0ilJj0naFhHPlnyGPTvQsCb27JsknYiIlyLiLUmHJG2t8H0AGlQl7DdJerXv9ali2WVsT9tesL1QYV0AKmr8Al1EzEmakziMB9pUZc9+WtK6vtcfK5YB6KAqYX9M0gbbn7T9QUnfkHS4nrIA1G3kw/iIeNv2XZL+KOk6Sfsj4pnaKgNQq5G73kZaGefsQOMa+VENgGsHYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJjHXKZuSzY8eOgW27du0q/ez69evrLabPyZMnS9vvvffe0vYDBw7UV8yYsGcHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSSYxXUMpqamStv37NlT2t5kfzOWduHChdL2iYmJMVVy9QbN4lrpRzW2T0o6L+kdSW9HxMYq3wegOXX8gu7LEfF6Dd8DoEGcswNJVA17SPqT7cdtTy/1BtvTthdsL1RcF4AKqh7Gb46I07Y/KumI7X9GxLH+N0TEnKQ5Ke8FOqALKu3ZI+J08XhW0u8kbaqjKAD1Gznstq+3/eFLzyV9TdLxugoDUK+R+9ltf0q9vbnUOx34dUT8aMhnUh7Gv/zyy6Xt9KNfe+wlu7I7ofZ+9oh4SdLnRq4IwFjR9QYkQdiBJAg7kARhB5Ig7EAS3Ep6DPbt21faPjs7O6ZKuuXQoUOl7Y888khp+8MPP1zaPjk5edU1rWTs2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCW4ljc5aXFwsbb/xxhsbW/fevXtL22dmZhpbd1WDhriyZweSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJOhnR2c1+d/mtdyPPgz97EByhB1IgrADSRB2IAnCDiRB2IEkCDuQBPeNR2vuvPPO1tZ9Lfejj2ront32fttnbR/vW7ba9hHbLxaPq5otE0BVyzmMPyBp6oplOyUdjYgNko4WrwF02NCwR8QxSeeuWLxV0sHi+UFJt9ZcF4CajXrOviYiLt0g7DVJawa90fa0pOkR1wOgJpUv0EVElA1wiYg5SXMSA2GANo3a9XbG9lpJKh7P1lcSgCaMGvbDkrYXz7dL+n095QBoytDDeNvzkm6RdIPtU5J2SbpP0m9sf1PSK5Jub7JIXLumpq7syHlX0/PSD5v/PZuhYY+IbQOavlJzLQAaxM9lgSQIO5AEYQeSIOxAEoQdSIJbSaNRZdMuNznlsiRNTEwMbLtw4UKj624Tt5IGkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4lTQqGXY76Cb70odNu7yS+9JHwZ4dSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgPDsqKRuvLlXrZx92K+ht2wbd+Dg3xrMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBKMZ0epNser33HHHY19d0ZD9+y299s+a/t437Ldtk/bfrL429JsmQCqWs5h/AFJU0ssfyAibi7+/lBvWQDqNjTsEXFM0rkx1AKgQVUu0N1l+6niMH/VoDfZnra9YHuhwroAVDRq2PdI+rSkmyUtSrp/0BsjYi4iNkbExhHXBaAGI4U9Is5ExDsRcVHSg5I21VsWgLqNFHbba/te3ibp+KD3AuiGof3stucl3SLpBtunJO2SdIvtmyWFpJOSvt1gjWjQ1NRSHS3vmp2dbWzdw8arc9/3eg0Ne0QsdYeAhxqoBUCD+LkskARhB5Ig7EAShB1IgrADSXAr6RVuWNfa/Px8afvk5GSd5VxmYmKitJ2ut9FwK2kgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJbSa8AZX3pbfajS9LevXsHttGPPl7s2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcazrwCLi4sD25qcUlkq70eXpJmZmUbXj/diPDuQHGEHkiDsQBKEHUiCsANJEHYgCcIOJMF49mvAsHu/N9mXTj/6yjF0z257ne2/2H7W9jO2v1ssX237iO0Xi8dVzZcLYFTLOYx/W9L3IuIzkr4o6Tu2PyNpp6SjEbFB0tHiNYCOGhr2iFiMiCeK5+clPSfpJklbJR0s3nZQ0q1NFQmguqs6Z7e9XtLnJf1d0pqIuPSj7NckrRnwmWlJ06OXCKAOy74ab/tDkh6VdHdEvNHfFr3RNEsOcomIuYjYGBEbK1UKoJJlhd32B9QL+q8i4rfF4jO21xbtayWdbaZEAHUYOsTVttU7Jz8XEXf3Lf+JpP9ExH22d0paHRHfH/JdDHEdQdkQVqnZrrfePz+uJYOGuC4n7Jsl/VXS05IuFovvUe+8/TeSPi7pFUm3R8S5Id9F2EdA2HE1BoV96AW6iPibpEH/4l+pUhSA8eHnskAShB1IgrADSRB2IAnCDiTBENcO6PIQVqwc7NmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAmmbB6DYf3o8/Pzpe2Tk5N1lnMZhrCuPEzZDCRH2IEkCDuQBGEHkiDsQBKEHUiCsANJMJ59DPbs2VPa3mQ/OuPVcQl7diAJwg4kQdiBJAg7kARhB5Ig7EAShB1IYmg/u+11kn4haY2kkDQXET+3vVvStyT9u3jrPRHxh6YKvZbt27evtH12drbS95f1pc/MzFT6bqwcy/lRzduSvhcRT9j+sKTHbR8p2h6IiJ82Vx6AuixnfvZFSYvF8/O2n5N0U9OFAajXVZ2z214v6fOS/l4susv2U7b321414DPTthdsL1SqFEAlyw677Q9JelTS3RHxhqQ9kj4t6Wb19vz3L/W5iJiLiI0RsbGGegGMaFlht/0B9YL+q4j4rSRFxJmIeCciLkp6UNKm5soEUNXQsLt3+9GHJD0XET/rW7627223STpef3kA6jL0VtK2N0v6q6SnJV0sFt8jaZt6h/Ah6aSkbxcX88q+K+WtpIFxGnQrae4bD6ww3DceSI6wA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQxLinbH5d0it9r28olnVRV2vral0StY2qzto+MahhrOPZ37Nye6Gr96bram1drUuitlGNqzYO44EkCDuQRNthn2t5/WW6WltX65KobVRjqa3Vc3YA49P2nh3AmBB2IIlWwm57yvbztk/Y3tlGDYPYPmn7adtPtj0/XTGH3lnbx/uWrbZ9xPaLxeOSc+y1VNtu26eLbfek7S0t1bbO9l9sP2v7GdvfLZa3uu1K6hrLdhv7Obvt6yS9IOmrkk5JekzStoh4dqyFDGD7pKSNEdH6DzBsf0nSm5J+ERGfLZb9WNK5iLiv+B/lqoj4QUdq2y3pzban8S5mK1rbP824pFsl7VCL266krts1hu3Wxp59k6QTEfFSRLwl6ZCkrS3U0XkRcUzSuSsWb5V0sHh+UL3/WMZuQG2dEBGLEfFE8fy8pEvTjLe67UrqGos2wn6TpFf7Xp9St+Z7D0l/sv247em2i1nCmr5ptl6TtKbNYpYwdBrvcbpimvHObLtRpj+vigt077U5Ir4g6euSvlMcrnZS9M7ButR3uqxpvMdliWnG/6/NbTfq9OdVtRH205LW9b3+WLGsEyLidPF4VtLv1L2pqM9cmkG3eDzbcj3/16VpvJeaZlwd2HZtTn/eRtgfk7TB9idtf1DSNyQdbqGO97B9fXHhRLavl/Q1dW8q6sOSthfPt0v6fYu1XKYr03gPmmZcLW+71qc/j4ix/0naot4V+X9J+mEbNQyo61OS/lH8PdN2bZLm1Tus+6961za+Kekjko5KelHSnyWt7lBtv1Rvau+n1AvW2pZq26zeIfpTkp4s/ra0ve1K6hrLduPnskASXKADkiDsQBKEHUiCsANJEHYgCcIOJEHYgST+BwiGD2n5FQUrAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuOFcXmV12jY",
        "outputId": "41515f91-979d-4b6b-a241-a0a2713145ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "output = model(transform(img).to(device).reshape(1,1,28,28))\n",
        "output.argmax()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7, device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM-N1wGBjgxN"
      },
      "source": [
        "## Sieci konwolucyjne\n",
        "\n",
        "![](https://miro.medium.com/max/500/1*YvlCSNzDEBGEWkZWNffPvw.gif)\n",
        "![](https://miro.medium.com/max/500/1*gXAcHnbTxmPb8KjSryki-g.gif)\n",
        "![](https://miro.medium.com/max/500/1*34_365CJB5seboQDUrbI5A.gif)\n",
        "![](https://miro.medium.com/max/500/1*WpOcRWlofm0Z0EDUTKefzg.gif)\n",
        "\n",
        "## Max pooling\n",
        "\n",
        "![](https://miro.medium.com/max/500/1*kW4HcS4zFxoKv6R4xtqFlg.gif)\n",
        "![](https://miro.medium.com/max/500/1*LjXV6eQKTQcg-PJnBRE0VA.gif)\n",
        "\n",
        "Źródło ilustracji: https://towardsdatascience.com/a-visualization-of-the-basic-elements-of-a-convolutional-neural-network-75fea30cd78d\n",
        "\n",
        "## Więcej kanałów?\n",
        "\n",
        "![](https://wiki.pathmind.com/images/wiki/karpathy-convnet-labels.png)\n",
        "\n",
        "Źródło: https://wiki.pathmind.com/images/wiki/karpathy-convnet-labels.png\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZgChyLGqVwu"
      },
      "source": [
        "## Zadanie 5.\n",
        "Napisz funkcję, która dla podanych wartości wysokości, szerokości, rozmiaru filtra, paddingu i stride'u zwróci rozmiar wyjściowy. Zakładamy dzielenie całkowite (//) w sytuacji gdy mielibyśmy uzyskać wynik niecałkowity - jak kolejne umieszczenie filtra się nie zmieści, to pomijamy je."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTUvNkW7uKCQ",
        "outputId": "97eb919d-d4a8-4d2b-f9ca-21276a866e86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def calc_result(height: int, width: int, kernel_size: int, padding: int, stride: int):\n",
        "    output_height = (height + 2 * padding - kernel_size) // stride + 1\n",
        "    output_width = (width + 2 * padding - kernel_size) // stride + 1\n",
        "    return output_height, output_width\n",
        "\n",
        "assert calc_result(4, 4, 2, 0, 1) == (3,3)\n",
        "assert calc_result(10, 8, 3, 1, 2) == (5,4)\n",
        "assert calc_result(2, 6, 2, 1, 2) == (2,4)\n",
        "calc_result(26,26,3,0,1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXSt0Rf9p3NF"
      },
      "source": [
        "## Zadanie 6.\n",
        "Uzyskujemy wyniki powyżej 90%, to możemy potranować na czymś trudniejszym. Użyjmy zbioru CIFAR10, jest to zbiór zdjęć 32x32 kolorowych (3 kanały: czerwony, niebieski i zielony) podzielonych na 10 klas. Dostosuj poniższy model tak, aby umożliwić trening na tym zbiorze - podobnie jak dla MNIST są gotowe klasy pobierające ten zbiór.\n",
        "\n",
        "Wskazówka: parametry do normalizacji: (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKTUv7puqL60"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CyvRjBhgS-a",
        "outputId": "dbeb29e0-a87c-45a1-ee87-10936c6a4baa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "device = \"cuda\"\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 16, 3, 1)\n",
        "        self.pool = nn.MaxPool2d(3, 3)\n",
        "        self.fc1 = nn.Linear(1296, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = torch.flatten(x,1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.CIFAR10('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.CIFAR10('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=100)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=100)\n",
        "\n",
        "model = Network().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.287536\n",
            "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 2.301362\n",
            "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 2.302276\n",
            "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 2.297142\n",
            "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 2.311600\n",
            "\n",
            "Test set: Average loss: 2.3032, Accuracy: 1000/10000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 2.309360\n",
            "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 2.298872\n",
            "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 2.302567\n",
            "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 2.297497\n",
            "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 2.311203\n",
            "\n",
            "Test set: Average loss: 2.3033, Accuracy: 1000/10000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 2.309851\n",
            "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 2.298931\n",
            "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 2.302460\n",
            "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 2.297496\n",
            "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 2.311224\n",
            "\n",
            "Test set: Average loss: 2.3033, Accuracy: 1000/10000 (10%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oPE2ni9y3VV"
      },
      "source": [
        "## Autoenkodery\n",
        "Są to sieci o specjalnej architekturze - rozmiar ich wejścia jest równy rozmiarowi wyjścia.\n",
        "\n",
        "![](https://miro.medium.com/max/1000/0*uq2_ZipB9TqI9G_k)\n",
        "\n",
        "Źródło ilustracji: https://miro.medium.com/max/1000/0*uq2_ZipB9TqI9G_k\n",
        "\n",
        "Można je zastosować m. in. do kompresji (stratnej), detekcji anomalii, czy część zwaną dekoderem wykorzystać do generowania nowych przykładów."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfpTnEJgzcTb"
      },
      "source": [
        "## Zadanie 7.\n",
        "Sprawdź działanie poniższego autoenkodera na przykładzie swojej cyfry - spróbój wyświetlić odtworzoną cyfrę."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhGwPQ9s5HeV",
        "outputId": "382eccf6-569f-4b0a-c779-663924f48003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.fc0 = nn.Linear(28*28, 300)\n",
        "        self.fc1 = nn.Linear(300, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.fc3 = nn.Linear(10, 4)\n",
        "        self.fc4 = nn.Linear(4, 10)\n",
        "        self.fc5 = nn.Linear(10, 128)\n",
        "        self.fc6 = nn.Linear(128, 300)\n",
        "        self.fc7 = nn.Linear(300, 28*28)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc0(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc6(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc7(x)\n",
        "        output = torch.reshape(x, (-1, 1, 28,28))\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    log_interval = 10\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.mse_loss(output, data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.cuda()\n",
        "            output = model(data)\n",
        "            test_loss += F.mse_loss(output, data).item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f})\\n'.format(test_loss))\n",
        "\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1, batch_size=500)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, batch_size=500)\n",
        "\n",
        "model = Network().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, train_loader, optimizer, epoch)\n",
        "    test(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.972252\n",
            "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.853668\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.781324\n",
            "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.669206\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.727288\n",
            "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.734077\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.704839\n",
            "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.660470\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.652307\n",
            "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.635920\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.578919\n",
            "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.566503\n",
            "\n",
            "Test set: Average loss: 0.0012)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.553821\n",
            "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.577829\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.588549\n",
            "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.546305\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.570592\n",
            "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.558130\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.559335\n",
            "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.552554\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.535393\n",
            "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.535983\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.516030\n",
            "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.505974\n",
            "\n",
            "Test set: Average loss: 0.0011)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.497996\n",
            "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.528971\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.544928\n",
            "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.502789\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.518077\n",
            "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.516524\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.519620\n",
            "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.512532\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.507141\n",
            "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.510179\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.494375\n",
            "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.477575\n",
            "\n",
            "Test set: Average loss: 0.0010)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.470155\n",
            "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.499848\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.510259\n",
            "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.472993\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.495088\n",
            "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.488899\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.488923\n",
            "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.479973\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.478207\n",
            "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.475678\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.467979\n",
            "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.447146\n",
            "\n",
            "Test set: Average loss: 0.0009)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.438767\n",
            "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.470972\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.481591\n",
            "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.455404\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.474855\n",
            "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.467352\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.476575\n",
            "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.466527\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.462931\n",
            "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.464508\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.454939\n",
            "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.433528\n",
            "\n",
            "Test set: Average loss: 0.0009)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.421798\n",
            "Train Epoch: 6 [5000/60000 (8%)]\tLoss: 0.456058\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 0.466895\n",
            "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 0.440242\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.459040\n",
            "Train Epoch: 6 [25000/60000 (42%)]\tLoss: 0.449453\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 0.460618\n",
            "Train Epoch: 6 [35000/60000 (58%)]\tLoss: 0.449046\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.450627\n",
            "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 0.454842\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 0.447732\n",
            "Train Epoch: 6 [55000/60000 (92%)]\tLoss: 0.423484\n",
            "\n",
            "Test set: Average loss: 0.0009)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.412282\n",
            "Train Epoch: 7 [5000/60000 (8%)]\tLoss: 0.446959\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 0.456287\n",
            "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 0.429557\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.446921\n",
            "Train Epoch: 7 [25000/60000 (42%)]\tLoss: 0.439773\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 0.451160\n",
            "Train Epoch: 7 [35000/60000 (58%)]\tLoss: 0.436461\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.442008\n",
            "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 0.444995\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 0.437711\n",
            "Train Epoch: 7 [55000/60000 (92%)]\tLoss: 0.418009\n",
            "\n",
            "Test set: Average loss: 0.0009)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.406242\n",
            "Train Epoch: 8 [5000/60000 (8%)]\tLoss: 0.439760\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 0.450500\n",
            "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 0.422698\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.442796\n",
            "Train Epoch: 8 [25000/60000 (42%)]\tLoss: 0.431180\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 0.447423\n",
            "Train Epoch: 8 [35000/60000 (58%)]\tLoss: 0.430326\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.433548\n",
            "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 0.438539\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 0.431530\n",
            "Train Epoch: 8 [55000/60000 (92%)]\tLoss: 0.410511\n",
            "\n",
            "Test set: Average loss: 0.0009)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.400008\n",
            "Train Epoch: 9 [5000/60000 (8%)]\tLoss: 0.430217\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 0.446007\n",
            "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 0.415473\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.432921\n",
            "Train Epoch: 9 [25000/60000 (42%)]\tLoss: 0.422493\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.438119\n",
            "Train Epoch: 9 [35000/60000 (58%)]\tLoss: 0.430681\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.430111\n",
            "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 0.433665\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 0.425930\n",
            "Train Epoch: 9 [55000/60000 (92%)]\tLoss: 0.404626\n",
            "\n",
            "Test set: Average loss: 0.0009)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.397894\n",
            "Train Epoch: 10 [5000/60000 (8%)]\tLoss: 0.427930\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.445645\n",
            "Train Epoch: 10 [15000/60000 (25%)]\tLoss: 0.412012\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 0.426066\n",
            "Train Epoch: 10 [25000/60000 (42%)]\tLoss: 0.420246\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 0.429987\n",
            "Train Epoch: 10 [35000/60000 (58%)]\tLoss: 0.420787\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 0.423142\n",
            "Train Epoch: 10 [45000/60000 (75%)]\tLoss: 0.428240\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 0.423286\n",
            "Train Epoch: 10 [55000/60000 (92%)]\tLoss: 0.402563\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.391100\n",
            "Train Epoch: 11 [5000/60000 (8%)]\tLoss: 0.420430\n",
            "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 0.443250\n",
            "Train Epoch: 11 [15000/60000 (25%)]\tLoss: 0.406080\n",
            "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.427724\n",
            "Train Epoch: 11 [25000/60000 (42%)]\tLoss: 0.416640\n",
            "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 0.423893\n",
            "Train Epoch: 11 [35000/60000 (58%)]\tLoss: 0.418021\n",
            "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 0.420923\n",
            "Train Epoch: 11 [45000/60000 (75%)]\tLoss: 0.426776\n",
            "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 0.419056\n",
            "Train Epoch: 11 [55000/60000 (92%)]\tLoss: 0.400965\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.388589\n",
            "Train Epoch: 12 [5000/60000 (8%)]\tLoss: 0.416402\n",
            "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 0.437766\n",
            "Train Epoch: 12 [15000/60000 (25%)]\tLoss: 0.402573\n",
            "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 0.421868\n",
            "Train Epoch: 12 [25000/60000 (42%)]\tLoss: 0.410003\n",
            "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 0.419624\n",
            "Train Epoch: 12 [35000/60000 (58%)]\tLoss: 0.412183\n",
            "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 0.418669\n",
            "Train Epoch: 12 [45000/60000 (75%)]\tLoss: 0.425169\n",
            "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 0.421381\n",
            "Train Epoch: 12 [55000/60000 (92%)]\tLoss: 0.397271\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.390631\n",
            "Train Epoch: 13 [5000/60000 (8%)]\tLoss: 0.417114\n",
            "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 0.434967\n",
            "Train Epoch: 13 [15000/60000 (25%)]\tLoss: 0.400956\n",
            "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.416476\n",
            "Train Epoch: 13 [25000/60000 (42%)]\tLoss: 0.406484\n",
            "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 0.417440\n",
            "Train Epoch: 13 [35000/60000 (58%)]\tLoss: 0.407665\n",
            "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 0.413255\n",
            "Train Epoch: 13 [45000/60000 (75%)]\tLoss: 0.421672\n",
            "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 0.415168\n",
            "Train Epoch: 13 [55000/60000 (92%)]\tLoss: 0.393795\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.386339\n",
            "Train Epoch: 14 [5000/60000 (8%)]\tLoss: 0.415491\n",
            "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 0.430580\n",
            "Train Epoch: 14 [15000/60000 (25%)]\tLoss: 0.397301\n",
            "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 0.416802\n",
            "Train Epoch: 14 [25000/60000 (42%)]\tLoss: 0.404011\n",
            "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 0.414836\n",
            "Train Epoch: 14 [35000/60000 (58%)]\tLoss: 0.404101\n",
            "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 0.410829\n",
            "Train Epoch: 14 [45000/60000 (75%)]\tLoss: 0.419245\n",
            "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 0.414441\n",
            "Train Epoch: 14 [55000/60000 (92%)]\tLoss: 0.392997\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.381613\n",
            "Train Epoch: 15 [5000/60000 (8%)]\tLoss: 0.410510\n",
            "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 0.433633\n",
            "Train Epoch: 15 [15000/60000 (25%)]\tLoss: 0.395554\n",
            "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 0.409774\n",
            "Train Epoch: 15 [25000/60000 (42%)]\tLoss: 0.403850\n",
            "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 0.412840\n",
            "Train Epoch: 15 [35000/60000 (58%)]\tLoss: 0.406732\n",
            "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.409815\n",
            "Train Epoch: 15 [45000/60000 (75%)]\tLoss: 0.420541\n",
            "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 0.414809\n",
            "Train Epoch: 15 [55000/60000 (92%)]\tLoss: 0.392984\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.378969\n",
            "Train Epoch: 16 [5000/60000 (8%)]\tLoss: 0.413007\n",
            "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 0.425612\n",
            "Train Epoch: 16 [15000/60000 (25%)]\tLoss: 0.392464\n",
            "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 0.408437\n",
            "Train Epoch: 16 [25000/60000 (42%)]\tLoss: 0.399834\n",
            "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 0.411863\n",
            "Train Epoch: 16 [35000/60000 (58%)]\tLoss: 0.401553\n",
            "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.409443\n",
            "Train Epoch: 16 [45000/60000 (75%)]\tLoss: 0.418371\n",
            "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 0.408636\n",
            "Train Epoch: 16 [55000/60000 (92%)]\tLoss: 0.391368\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.378281\n",
            "Train Epoch: 17 [5000/60000 (8%)]\tLoss: 0.407430\n",
            "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 0.421919\n",
            "Train Epoch: 17 [15000/60000 (25%)]\tLoss: 0.392319\n",
            "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 0.405484\n",
            "Train Epoch: 17 [25000/60000 (42%)]\tLoss: 0.398966\n",
            "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 0.408071\n",
            "Train Epoch: 17 [35000/60000 (58%)]\tLoss: 0.399420\n",
            "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 0.406956\n",
            "Train Epoch: 17 [45000/60000 (75%)]\tLoss: 0.418013\n",
            "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 0.408198\n",
            "Train Epoch: 17 [55000/60000 (92%)]\tLoss: 0.390435\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.377998\n",
            "Train Epoch: 18 [5000/60000 (8%)]\tLoss: 0.406550\n",
            "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 0.422678\n",
            "Train Epoch: 18 [15000/60000 (25%)]\tLoss: 0.389537\n",
            "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 0.402588\n",
            "Train Epoch: 18 [25000/60000 (42%)]\tLoss: 0.400350\n",
            "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 0.406544\n",
            "Train Epoch: 18 [35000/60000 (58%)]\tLoss: 0.397090\n",
            "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.404490\n",
            "Train Epoch: 18 [45000/60000 (75%)]\tLoss: 0.414877\n",
            "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 0.407227\n",
            "Train Epoch: 18 [55000/60000 (92%)]\tLoss: 0.385791\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.375666\n",
            "Train Epoch: 19 [5000/60000 (8%)]\tLoss: 0.400173\n",
            "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 0.420677\n",
            "Train Epoch: 19 [15000/60000 (25%)]\tLoss: 0.387780\n",
            "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 0.403600\n",
            "Train Epoch: 19 [25000/60000 (42%)]\tLoss: 0.396121\n",
            "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 0.406374\n",
            "Train Epoch: 19 [35000/60000 (58%)]\tLoss: 0.391629\n",
            "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.401160\n",
            "Train Epoch: 19 [45000/60000 (75%)]\tLoss: 0.411235\n",
            "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 0.401775\n",
            "Train Epoch: 19 [55000/60000 (92%)]\tLoss: 0.383423\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.376334\n",
            "Train Epoch: 20 [5000/60000 (8%)]\tLoss: 0.404677\n",
            "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 0.419471\n",
            "Train Epoch: 20 [15000/60000 (25%)]\tLoss: 0.388523\n",
            "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 0.400835\n",
            "Train Epoch: 20 [25000/60000 (42%)]\tLoss: 0.393141\n",
            "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 0.405598\n",
            "Train Epoch: 20 [35000/60000 (58%)]\tLoss: 0.391640\n",
            "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 0.401185\n",
            "Train Epoch: 20 [45000/60000 (75%)]\tLoss: 0.411952\n",
            "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 0.403748\n",
            "Train Epoch: 20 [55000/60000 (92%)]\tLoss: 0.378054\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.368138\n",
            "Train Epoch: 21 [5000/60000 (8%)]\tLoss: 0.393454\n",
            "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 0.413493\n",
            "Train Epoch: 21 [15000/60000 (25%)]\tLoss: 0.383078\n",
            "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 0.395694\n",
            "Train Epoch: 21 [25000/60000 (42%)]\tLoss: 0.391678\n",
            "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 0.403330\n",
            "Train Epoch: 21 [35000/60000 (58%)]\tLoss: 0.387335\n",
            "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 0.399305\n",
            "Train Epoch: 21 [45000/60000 (75%)]\tLoss: 0.412031\n",
            "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 0.397320\n",
            "Train Epoch: 21 [55000/60000 (92%)]\tLoss: 0.374849\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.364937\n",
            "Train Epoch: 22 [5000/60000 (8%)]\tLoss: 0.390326\n",
            "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 0.414228\n",
            "Train Epoch: 22 [15000/60000 (25%)]\tLoss: 0.380831\n",
            "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 0.393088\n",
            "Train Epoch: 22 [25000/60000 (42%)]\tLoss: 0.390843\n",
            "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 0.398263\n",
            "Train Epoch: 22 [35000/60000 (58%)]\tLoss: 0.384161\n",
            "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 0.393160\n",
            "Train Epoch: 22 [45000/60000 (75%)]\tLoss: 0.403733\n",
            "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 0.394034\n",
            "Train Epoch: 22 [55000/60000 (92%)]\tLoss: 0.371205\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.362159\n",
            "Train Epoch: 23 [5000/60000 (8%)]\tLoss: 0.392088\n",
            "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 0.407461\n",
            "Train Epoch: 23 [15000/60000 (25%)]\tLoss: 0.378996\n",
            "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 0.389812\n",
            "Train Epoch: 23 [25000/60000 (42%)]\tLoss: 0.386306\n",
            "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 0.399438\n",
            "Train Epoch: 23 [35000/60000 (58%)]\tLoss: 0.382257\n",
            "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 0.389098\n",
            "Train Epoch: 23 [45000/60000 (75%)]\tLoss: 0.400005\n",
            "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 0.389393\n",
            "Train Epoch: 23 [55000/60000 (92%)]\tLoss: 0.366180\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.362686\n",
            "Train Epoch: 24 [5000/60000 (8%)]\tLoss: 0.387619\n",
            "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 0.401700\n",
            "Train Epoch: 24 [15000/60000 (25%)]\tLoss: 0.374135\n",
            "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 0.389439\n",
            "Train Epoch: 24 [25000/60000 (42%)]\tLoss: 0.383418\n",
            "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 0.390143\n",
            "Train Epoch: 24 [35000/60000 (58%)]\tLoss: 0.378420\n",
            "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 0.385364\n",
            "Train Epoch: 24 [45000/60000 (75%)]\tLoss: 0.396682\n",
            "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 0.385693\n",
            "Train Epoch: 24 [55000/60000 (92%)]\tLoss: 0.363623\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.357038\n",
            "Train Epoch: 25 [5000/60000 (8%)]\tLoss: 0.382536\n",
            "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 0.396743\n",
            "Train Epoch: 25 [15000/60000 (25%)]\tLoss: 0.371629\n",
            "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 0.388329\n",
            "Train Epoch: 25 [25000/60000 (42%)]\tLoss: 0.379957\n",
            "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 0.387086\n",
            "Train Epoch: 25 [35000/60000 (58%)]\tLoss: 0.373509\n",
            "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 0.383518\n",
            "Train Epoch: 25 [45000/60000 (75%)]\tLoss: 0.391939\n",
            "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 0.379892\n",
            "Train Epoch: 25 [55000/60000 (92%)]\tLoss: 0.362699\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.360227\n",
            "Train Epoch: 26 [5000/60000 (8%)]\tLoss: 0.383250\n",
            "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 0.393277\n",
            "Train Epoch: 26 [15000/60000 (25%)]\tLoss: 0.370475\n",
            "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 0.384188\n",
            "Train Epoch: 26 [25000/60000 (42%)]\tLoss: 0.377661\n",
            "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 0.385431\n",
            "Train Epoch: 26 [35000/60000 (58%)]\tLoss: 0.373034\n",
            "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 0.381458\n",
            "Train Epoch: 26 [45000/60000 (75%)]\tLoss: 0.392582\n",
            "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 0.377170\n",
            "Train Epoch: 26 [55000/60000 (92%)]\tLoss: 0.359344\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.356211\n",
            "Train Epoch: 27 [5000/60000 (8%)]\tLoss: 0.379748\n",
            "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 0.389973\n",
            "Train Epoch: 27 [15000/60000 (25%)]\tLoss: 0.365562\n",
            "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 0.381925\n",
            "Train Epoch: 27 [25000/60000 (42%)]\tLoss: 0.378786\n",
            "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 0.383848\n",
            "Train Epoch: 27 [35000/60000 (58%)]\tLoss: 0.371011\n",
            "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 0.382018\n",
            "Train Epoch: 27 [45000/60000 (75%)]\tLoss: 0.390132\n",
            "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 0.374163\n",
            "Train Epoch: 27 [55000/60000 (92%)]\tLoss: 0.358407\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.353726\n",
            "Train Epoch: 28 [5000/60000 (8%)]\tLoss: 0.376447\n",
            "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 0.389891\n",
            "Train Epoch: 28 [15000/60000 (25%)]\tLoss: 0.364655\n",
            "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 0.375445\n",
            "Train Epoch: 28 [25000/60000 (42%)]\tLoss: 0.375907\n",
            "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 0.383889\n",
            "Train Epoch: 28 [35000/60000 (58%)]\tLoss: 0.370098\n",
            "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 0.378094\n",
            "Train Epoch: 28 [45000/60000 (75%)]\tLoss: 0.389103\n",
            "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 0.373886\n",
            "Train Epoch: 28 [55000/60000 (92%)]\tLoss: 0.358365\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.353203\n",
            "Train Epoch: 29 [5000/60000 (8%)]\tLoss: 0.371131\n",
            "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 0.387459\n",
            "Train Epoch: 29 [15000/60000 (25%)]\tLoss: 0.363720\n",
            "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 0.376297\n",
            "Train Epoch: 29 [25000/60000 (42%)]\tLoss: 0.372762\n",
            "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 0.381430\n",
            "Train Epoch: 29 [35000/60000 (58%)]\tLoss: 0.367753\n",
            "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 0.375927\n",
            "Train Epoch: 29 [45000/60000 (75%)]\tLoss: 0.390551\n",
            "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 0.372191\n",
            "Train Epoch: 29 [55000/60000 (92%)]\tLoss: 0.355789\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.353281\n",
            "Train Epoch: 30 [5000/60000 (8%)]\tLoss: 0.373524\n",
            "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 0.386254\n",
            "Train Epoch: 30 [15000/60000 (25%)]\tLoss: 0.363050\n",
            "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 0.374161\n",
            "Train Epoch: 30 [25000/60000 (42%)]\tLoss: 0.372653\n",
            "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 0.381200\n",
            "Train Epoch: 30 [35000/60000 (58%)]\tLoss: 0.370066\n",
            "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 0.374673\n",
            "Train Epoch: 30 [45000/60000 (75%)]\tLoss: 0.387567\n",
            "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 0.370720\n",
            "Train Epoch: 30 [55000/60000 (92%)]\tLoss: 0.355430\n",
            "\n",
            "Test set: Average loss: 0.0008)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdJNr4qmyQin",
        "outputId": "97dcb18a-7305-4e11-c3b4-e26212071573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "output = model(transform(img).to(device).reshape(1,1,28,28))\n",
        "mean = 0.1307\n",
        "std = 0.3081\n",
        "output = ((output * std) + mean) * 255\n",
        "plt.imshow(np.reshape(output.cpu().detach().numpy(), (28,28)), cmap=plt.cm.gray)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd9ccd8bef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSIKAyXOz0Ai"
      },
      "source": [
        "## Co dalej?\n",
        "Jeżeli zainteresował Cię temat sieci neuronowych możesz poszukać jeszcze informacji na poniższe tematy:\n",
        " - regularyzacja - czyli jak przeciwdziałać overfittingowi (m. in. dropout, L1, L2, batch normalization),\n",
        " - sieci rekurencyjne - czyli jak przyjmować dane o różnej wielkości,\n",
        " - VAE (wariacyjny autoenkoder), GAN - czyli jak generować nowe obrazy na podstawie danego zbioru (lepiej niż za pomocą zwykłego autoenkodera),\n",
        " - przetwarzanie języka naturalnego - embeddingi: Glove, Word2Vec; model atencji, BERT - czyli jak wyciągnąć coś z tekstu (np. analiza sentymentu - czy dany tekst jest określony pozytywnie, czy negatywnie),\n",
        " itp."
      ]
    }
  ]
}